---
sidebar_position: 1
---

# Optimal Power Flow (OPF)


![Whiteboard of the OPF challenge](/img/OPF-challenge.jpg)

## The challenge

Rougly speaking, at each time $t$ the goal is to plan the conventional generation levels for the next time step $t+1$, minimising the shortage or surplus of power, given random demand and random power from a wind farm.

Our challenge network has 3 buses. Bus 1 contains a conventional generator $G_1$ and a wind generator $W$. Bus 2 is a conventional generator $G_2$ and bus 3 is a consumer which generates a load $L$. 

![The 3 bus network](/img/3buses.jpg) 


## Starting point

There are 96 time steps, representing two days of half-hours. You are given the forecasts for wind generation $W(0),\ldots ,W(96)$ and demand forecasts $L(0),\ldots ,L(96)$. 

The conventional generators have maximum power levels of 100 MW and the cost of generating 1MW is £50 for generator 1 and £100 for generator 2. 

Based on these costs, predicted wind generation and load, and the network's constraints, an initial generation schedule for $G_1$ and $G_2$ is produced using an optimal power flow (OPF) solver.

In summary, you are given 
- wind generation forecasts $W(0),\ldots ,W(96)$,
- load forecasts $L(0),\ldots ,L(96)$,
- initial optimal generation levels $G_1(0),\ldots ,G_1(96)$ and $G_2(0),\ldots ,G_2(96)$.


## Step

At each time step the remaining forecasts are updated. So if the current time is $t$, then 
* $W(0),\ldots, W(t-1)$ are the levels generated by the wind farm at all past times, 
* $W(t)$ is the level it is generating at the current time, and 
* $W(t+1),\ldots, W(96)$ are the updated generation forecasts for the reamining future times.

At time $t$, your agent may choose to modify the generation planned for the next time step $t+1$ by an amount $\Delta G_1(t+1)$MW for generator 1 and an amount $\Delta G_2(t+1)$MW for generator 2, subject to the ramp rate contraints: the new generation levels $G_i(t+1)+\Delta G_i(t+1)$ must stay within 5MW of the current generation levels $G_i(t)+\Delta G_i(t)$.

## Cost 

The cost of the actions which you take at time $t$ is evaluated at time $t+1$. 
This cost $C(t+1)$ is the sum of the redispatch cost:

$C_{red}(t+1) = (\Delta G_1(t+1))^2 + (\Delta G_2(t+1))^2$

and the reserve cost. The reserve cost is higher if demand is larger than supply, requiring more fuel to be used (that is, if  $L(t+1) > G_1(t+1)+\Delta G_1(t+1) + G_2(t+1) + \Delta G_2(t+1) + W(t+1)$):

$C_{res}(t) = \big( G_1(t+1)+\Delta G_1(t+1) + G_2(t+1) + \Delta G_2(t+1) + W(t+1) - L(t+1) \big)^2$

and is lower if demand is lower than supply, resulting in a fuel saving (that is, if  $L(t+1) > G_1(t+1)+\Delta G_1(t+1) + G_2(t+1) + \Delta G_2(t+1) + W(t+1)$):

$C_{res}(t+1) = \frac{1}{4} \big( G_1(t+1)+\Delta G_1(t+1) + G_2(t+1) + \Delta G_2(t+1) + W(t+1) - L(t+1) \big)^2.$

## Setup instructions

Download the code: 

```shell
git clone https://gitlab.com/rangl/challenges/opf.git
```

Create a virtual enviornment with the required packages:

```
conda create -n openai python=3.7
source activate openai
pip install -r requirements.txt
```

Install the custom openAI gym environment for the opf challenge (notes on custom environments are [here](https://github.com/openai/gym/blob/master/docs/creating-environments.md)):

```
pip install -e opf-env
```

and import these packages:

```
import gym
import opf_env
from gym import error, spaces, utils
import random
import numpy as np
import matplotlib.pyplot as plt
```

then you can create an instance of the custom opf environment with 

```
env = gym.make('opf_env:ranglOpf-v5')
```

## Available methods

### step

The step method takes in two actions:
* action[0]
* action[1] 

in the range [-1,1]. These first need to be scaled to the range [-100,200] using this code:

```
dG1 = ((action[0] + 1) / 2.0) * 300 + -100.  # range [-100,200]
dG2 = ((action[1] + 1) / 2.0) * 300 + -100.
```

The step method returns 4 objects (see the gym documentation [here](https://gym.openai.com/docs/)):
* observation (current state of the environment)
* reward (negative of the cost associated to your action)
* done (bolean variable indicating if the time horizon T=96 has been reached)
* a dictionary containing additional info 

For example:
```
env.step([-1/3,-1/3])
```
corresponds to an agent which takes no action at time $t$, so at time $t+1$ the initial generation schedule will be followed (if this is permitted by the ramp rate constraints). 

Alternatively:
```
env.step([-1/3,-0.3])
``` 
corresponds to an agent which follows the initial generation schedule for generator 1 at time $t+1$, but increases the level of generator 2 by 5MW (again, if this is permitted by the ramp rate constraints).

If the action would violate the ramp rate constraints, then it saturates: it is set either as high or as low as possible within the ramp rate constraints, whichever is closer to the requested action.

### plot

After the actions have been taken, to visualise what happened run:

```
env.plot()
```

### reset

Go back to the initial setup of the environment with:
```
env.reset()
```

## Training

You can now train some RL algorithms from the Stable Baselines library [here](https://stable-baselines.readthedocs.io/en/master/):

```
from stable_baselines.common.env_checker import check_env
from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common.policies import FeedForwardPolicy
from stable_baselines.common.policies import MlpLstmPolicy
from stable_baselines import TRPO
```

For example, to train an agent using TRPO:

```
model = TRPO(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=250000)
```

The model can be saved and loaded with:
```
model.save("trpo_opf_1e6timesteps")
model = TRPO.load("trpo_opf_1e6timesteps")
```



<!--- Glossary comes at the bottom of the page --->
## Glossary 

* **Conventional generation**: Electricity generation, such as a gas turbine or hydropower plant, that can be controlled by choosing a generation level
* **Ramp rate**: A limit to the difference in the generation levels of consecutive time periods
* **Bus**: A node in the network, at which multiple features may be connected (eg. generation and load)
* **Redispatch cost**: The cost of changing the planned generation levels
* **Reserve cost**: A penalty for the mismatch between actual supply and demand, representing the cost of correcting this mismatch
* **TRPO**: A recent reinforcement learning algorithm




